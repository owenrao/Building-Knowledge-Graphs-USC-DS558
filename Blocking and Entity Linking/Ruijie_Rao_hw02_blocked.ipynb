{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rltk\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import re\n",
    "# You can use this tokenizer in case you need to manipulate some data\n",
    "tokenizer = rltk.tokenizer.crf_tokenizer.crf_tokenizer.CrfTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Entity Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Datase Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(doc):\n",
    "    return re.findall(r\"\\w+\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = ''\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def id(self):\n",
    "        return self.raw_object['ID']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def name_string(self):\n",
    "        return self.raw_object['Title']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def name_tokens(self):\n",
    "        return set(my_tokenizer(self.name_string))\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def ISBN(self):\n",
    "        return self.raw_object['ISBN']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def ISBN13(self):\n",
    "        if not self.raw_object['ISBN13'].strip():\n",
    "            return f\"Good Item {self.id} has no valid ISBN13 record\"\n",
    "        return self.raw_object['ISBN13']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def author(self):\n",
    "        return self.raw_object['FirstAuthor']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def publication_date(self):\n",
    "        date = self.raw_object['PublishDate']\n",
    "        try:\n",
    "            return parser.parse(date).date()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def publication_year(self):\n",
    "        try:\n",
    "            return str(self.publication_date.year)\n",
    "        except:\n",
    "            return f\"Good Item {self.id} has no valid publish year record\"\n",
    "    \n",
    "\n",
    "class NobleRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = ''\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def id(self):\n",
    "        return self.raw_object['ID']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def name_string(self):\n",
    "        return self.raw_object['Title']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def name_tokens(self):\n",
    "        return set(my_tokenizer(self.name_string))\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def ISBN13(self):\n",
    "        if not self.raw_object['ISBN13'].strip():\n",
    "            return f\"Noble Item {self.id} has no valid ISBN13 record\"\n",
    "        return self.raw_object['ISBN13']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def author(self):\n",
    "        return self.raw_object['Author1']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def publication_date(self):\n",
    "        date = self.raw_object['PublicationDate']\n",
    "        try:\n",
    "            return datetime.strptime(date,\"%m/%d/%Y\").date()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def publication_year(self):\n",
    "        try:\n",
    "            return str(self.publication_date.year)\n",
    "        except:\n",
    "            return f\"Noble Item {self.id} has no valid publish year record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_file_path = 'goodreads.csv'\n",
    "noble_file_path = 'barnes_and_nobles.csv'\n",
    "\n",
    "good_ds = rltk.Dataset(rltk.CSVReader(open(good_file_path, encoding=\"UTF8\")),record_class=GoodRecord)\n",
    "noble_ds = rltk.Dataset(rltk.CSVReader(open(noble_file_path, encoding=\"UTF8\")),record_class=NobleRecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = rltk.HashBlockGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are: goodreads.ID, barnes_and_nobles.ID, label\n",
      "Processed 297 lines.\n"
     ]
    }
   ],
   "source": [
    "dev_set_file = 'dev.csv'\n",
    "dev = []\n",
    "with open(dev_set_file, encoding='utf-8', errors=\"replace\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if len(row) <= 1:\n",
    "            continue\n",
    "        if line_count == 0:\n",
    "            columns = row\n",
    "            line_count += 1\n",
    "        else:\n",
    "            dev.append(row)\n",
    "    print(f'Column names are: {\", \".join(columns)}')\n",
    "    print(f'Processed {len(dev)} lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tokens_block = bg.generate(\n",
    "    bg.block(good_ds, function_=lambda r: \",\".join(r.name_tokens)),\n",
    "    bg.block(noble_ds, function_=lambda r: \",\".join(r.name_tokens))\n",
    ") #0.50,0.97\n",
    "\n",
    "year_block = bg.generate(\n",
    "    bg.block(good_ds, property_='publication_year'),\n",
    "    bg.block(noble_ds, property_='publication_year')\n",
    ") #0.23,0.80\n",
    "\n",
    "author_block = bg.generate(\n",
    "    bg.block(good_ds, property_='author', base_on=year_block),\n",
    "    bg.block(noble_ds, property_='author', base_on=year_block)\n",
    ") #0.69,0.79\n",
    "\n",
    "ISBN_block = bg.generate(\n",
    "    bg.block(good_ds, property_='ISBN13'),\n",
    "    bg.block(noble_ds, property_='ISBN13')\n",
    ") #0.12,0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pairs = list(rltk.get_record_pairs(good_ds, noble_ds, block=name_tokens_block))\n",
    "for pair in rltk.get_record_pairs(good_ds, noble_ds, block=author_block):\n",
    "    if pair not in candidate_pairs:\n",
    "        candidate_pairs.append(pair)\n",
    "for pair in rltk.get_record_pairs(good_ds, noble_ds, block=ISBN_block):\n",
    "    if pair not in candidate_pairs:\n",
    "        candidate_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rltk.evaluation.trial.Trial at 0x196e616a800>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = rltk.GroundTruth()\n",
    "C_ = 0\n",
    "Sm = 0\n",
    "Nm = 0\n",
    "for row in dev:    \n",
    "    r1 = good_ds.get_record(row[0])\n",
    "    r2  = noble_ds.get_record(row[1])\n",
    "    pair = (r1,r2)\n",
    "    if pair in candidate_pairs:\n",
    "        C_ += 1\n",
    "    if row[-1] == '1':\n",
    "        if pair in candidate_pairs:\n",
    "            Sm += 1\n",
    "        Nm += 1\n",
    "        gt.add_positive(r1.raw_object['ID'], r2.raw_object['ID'])\n",
    "    else:\n",
    "        gt.add_negative(r1.raw_object['ID'], r2.raw_object['ID'])\n",
    "\n",
    "rltk.Trial(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction Ratio: 0.0002757220155102152\n",
      "Pairwise Completeness: 0.9701492537313433\n",
      "Precision: 0.7386363636363636\n"
     ]
    }
   ],
   "source": [
    "N = 3700*3966\n",
    "C = len(candidate_pairs)\n",
    "RR = C/N\n",
    "PC = Sm/Nm\n",
    "precision = Sm/C_\n",
    "print(f\"Reduction Ratio: {RR}\")\n",
    "print(f\"Pairwise Completeness: {PC}\")\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ruijie_Rao_blocked.csv', 'w', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow((\"goodreads.ID\", \"barnes_and_nobles.ID\"))\n",
    "        for r1, r2 in candidate_pairs:\n",
    "            writer.writerow((r1.id, r2.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3: Entity Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_jaro_similarity(r1, r2):\n",
    "    s1 = r1.name_string\n",
    "    s2 = r2.name_string\n",
    "    \n",
    "    return rltk.jaro_winkler_similarity(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(vec1,vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_tokens_cos_similarity(r1, r2):\n",
    "    vec1 = Counter(r1.name_tokens)\n",
    "    vec2 = Counter(r2.name_tokens)\n",
    "    return cos_similarity(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_tokens_cos_similarity(r1, r2):\n",
    "    vec1 = Counter(my_tokenizer(r1.author))\n",
    "    vec2 = Counter(my_tokenizer(r2.author))\n",
    "    return cos_similarity(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ISBN_equality(r1, r2):\n",
    "    if r1.ISBN13 == r2.ISBN13:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_year_equality(r1, r2):\n",
    "    y1 = r1.publication_year\n",
    "    y2 = r2.publication_year\n",
    "    if y1 == y2:\n",
    "        return 0.5\n",
    "    else:\n",
    "        try:\n",
    "            _ = int(y1)\n",
    "            _ = int(y2)\n",
    "            return 0\n",
    "        except:\n",
    "            return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_TRESH = 0.83\n",
    "\n",
    "def rule_based_method(r1, r2):\n",
    "    if ISBN_equality(r1, r2)==1:\n",
    "        return True,1\n",
    "    score_1 = publish_year_equality(r1, r2)\n",
    "    score_2 = name_tokens_cos_similarity(r1, r2)\n",
    "    \n",
    "    total = 0.2 * score_1 + 0.75 * score_2\n",
    "    return total > MY_TRESH, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = rltk.Trial(gt)\n",
    "gt_candidate_pairs = rltk.get_record_pairs(good_ds, noble_ds, ground_truth=gt)\n",
    "for r1, r2 in gt_candidate_pairs:\n",
    "    result, confidence = rule_based_method(r1, r2)\n",
    "    trial.add_result(r1, r2, result, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial statistics based on Ground-Truth from development set data:\n",
      "precison: 0.9180327868852459 recall: 0.835820895522388 f-measure: 0.875\n",
      "tp: 0.835821 [56]\n",
      "fp: 0.021739 [5]\n",
      "tn: 0.978261 [225]\n",
      "fn: 0.164179 [11]\n"
     ]
    }
   ],
   "source": [
    "trial.evaluate()\n",
    "print('Trial statistics based on Ground-Truth from development set data:')\n",
    "print('precison:', trial.precision, 'recall:', trial.recall, 'f-measure:', trial.f_measure)\n",
    "print(f'tp: {trial.true_positives:.06f} [{len(trial.true_positives_list)}]')\n",
    "print(f'fp: {trial.false_positives:.06f} [{len(trial.false_positives_list)}]')\n",
    "print(f'tn: {trial.true_negatives:.06f} [{len(trial.true_negatives_list)}]')\n",
    "print(f'fn: {trial.false_negatives:.06f} [{len(trial.false_negatives_list)}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ruijie_Rao_predictions.csv', 'w', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow((\"goodreads.ID\", \"barnes_and_nobles.ID\", \"prediction\", \"confidence\"))\n",
    "        gt_candidate_pairs = rltk.get_record_pairs(good_ds, noble_ds, ground_truth=gt)\n",
    "        for r1, r2 in gt_candidate_pairs:\n",
    "            result, confidence = rule_based_method(r1, r2)\n",
    "            writer.writerow((r1.id, r2.id, int(result), confidence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pairs = list(rltk.get_record_pairs(good_ds, noble_ds, block=name_tokens_block))\n",
    "for pair in rltk.get_record_pairs(good_ds, noble_ds, block=author_block):\n",
    "    if pair not in candidate_pairs:\n",
    "        candidate_pairs.append(pair)\n",
    "for pair in rltk.get_record_pairs(good_ds, noble_ds, block=ISBN_block):\n",
    "    if pair not in candidate_pairs:\n",
    "        candidate_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ruijie_Rao_valid_predictions.csv', 'w', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow((\"goodreads.ID\", \"barnes_and_nobles.ID\"))\n",
    "        for r1, r2 in candidate_pairs:\n",
    "            result, confidence = rule_based_method(r1, r2)\n",
    "            if result:\n",
    "                writer.writerow((r1.id, r2.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Knowledge Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, Literal, XSD, Namespace, RDF, BNode, RDFS\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_books_id = set()\n",
    "noble_books_id = set()\n",
    "book_matchings_id = []\n",
    "with open(\"Ruijie_Rao_valid_predictions.csv\", encoding='utf-8', errors=\"replace\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    c = 0\n",
    "    for row in csv_reader:\n",
    "        if len(row) <= 1:\n",
    "            continue\n",
    "        if c==0:\n",
    "            c += 1\n",
    "            continue\n",
    "        good_books_id.add(row[0])\n",
    "        noble_books_id.add(row[1])\n",
    "        book_matchings_id.append((row[0],row[1]))\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOAF = Namespace('http://xmlns.com/foaf/0.1/')\n",
    "SCHEMA = Namespace('https://schema.org/')\n",
    "ISBN = Namespace('https://isbndb.com/book/')\n",
    "DBPD = Namespace('https://dbpedia.org/ontology/')\n",
    "MYNS = Namespace('http://dsci558.org/myfakenamespace#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kg = Graph()\n",
    "my_kg.bind('isbn', ISBN)\n",
    "my_kg.bind('foaf', FOAF)\n",
    "my_kg.bind('schema', SCHEMA)\n",
    "my_kg.bind('dbpedia', DBPD)\n",
    "my_kg.bind('myns', MYNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"goodreads.csv\", encoding='utf-8', errors=\"replace\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        if len(row) <= 1:\n",
    "            continue\n",
    "        if row[0] not in good_books_id:\n",
    "            continue\n",
    "        id = f\"good_{row[0]}\"\n",
    "        if row[4].strip(): subject = URIRef(ISBN[row[4]])\n",
    "        else: subject = MYNS[id]\n",
    "        my_kg.add((subject, SCHEMA.archivedAt, Literal(\"https://www.goodreads.com/\")))\n",
    "        my_kg.add((subject, RDF.type, SCHEMA.Book))\n",
    "        my_kg.add((subject, SCHEMA.name, Literal(row[1])))\n",
    "        my_kg.add((subject, SCHEMA.description, Literal(row[2])))\n",
    "        if row[3].strip(): my_kg.add((subject, SCHEMA.isbn, Literal(row[3])))\n",
    "        if row[4].strip(): my_kg.add((subject, SCHEMA.gtin13, Literal(row[4])))\n",
    "        my_kg.add((subject, SCHEMA.numberOfPages, Literal(int(row[5]), datatype=XSD.integer)))\n",
    "        for i in [6,7,8]:\n",
    "            if row[i].strip():\n",
    "                my_kg.add((subject, SCHEMA.author, Literal(row[i], datatype=SCHEMA.Person)))\n",
    "        # Rating\n",
    "        my_kg.add((BNode(\"rating_\"+id), RDF.type, SCHEMA.AggregateRating))\n",
    "        my_kg.add((BNode(\"rating_\"+id), SCHEMA.ratingValue, Literal(row[9], datatype=XSD.float)))\n",
    "        my_kg.add((BNode(\"rating_\"+id), SCHEMA.ratingCount, Literal(row[10], datatype=XSD.integer)))\n",
    "        my_kg.add((subject, SCHEMA.aggregateRating, BNode(\"rating_\"+id)))\n",
    "        # Reviews\n",
    "        my_kg.add((BNode(\"reviews_\"+id), RDF.type, SCHEMA.Review))\n",
    "        my_kg.add((BNode(\"reviews_\"+id), SCHEMA.commentCount, Literal(int(row[11].replace(\",\",\"\")), datatype=XSD.integer)))\n",
    "        my_kg.add((subject, SCHEMA.review, BNode(\"reviews_\"+id)))\n",
    "\n",
    "        if row[12].strip(): my_kg.add((subject, SCHEMA.publisher, Literal(row[12], datatype=SCHEMA.Organization)))\n",
    "        if row[13].strip(): my_kg.add((subject, SCHEMA.datePublished, Literal(row[13], datatype=SCHEMA.Date)))\n",
    "        # Format\n",
    "        if row[14].strip():  \n",
    "            if len((row[14].split(\" \")))==1:\n",
    "                my_kg.add((subject, SCHEMA.bookFormat, SCHEMA[row[14]]))\n",
    "            else:\n",
    "                my_kg.add((subject, SCHEMA.bookFormat, Literal(row[14])))\n",
    "        if row[15].strip():  my_kg.add((subject, SCHEMA.inLanguage, SCHEMA[row[15]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kg.add((MYNS[\"salesRank\"], RDF.type, RDF.Property))\n",
    "my_kg.add((MYNS[\"salesRank\"], RDFS.subPropertyOf, DBPD.rank))\n",
    "my_kg.add((MYNS[\"salesRank\"], RDFS.domain, SCHEMA.Book))\n",
    "my_kg.add((MYNS[\"salesRank\"], RDFS.range, XSD.integer))\n",
    "\n",
    "for price in [\"paperbackPrice\", \"hardcoverPrice\", \"nookBookPrice\", \"audiobookPrice\"]:\n",
    "    my_kg.add((MYNS[price], RDF.type, RDF.Property))\n",
    "    my_kg.add((MYNS[price], RDFS.subPropertyOf, SCHEMA.price))\n",
    "    my_kg.add((MYNS[price], RDFS.domain, SCHEMA.Book))\n",
    "    my_kg.add((MYNS[price], RDFS.range, XSD.dollar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"barnes_and_nobles.csv\", encoding='utf-8', errors=\"replace\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        if len(row) <= 1:\n",
    "            continue\n",
    "        if row[0] not in noble_books_id:\n",
    "            continue\n",
    "        if len(row)<17:\n",
    "            row = row + [\"\" for i in range(17-len(row))]\n",
    "        id = f\"noble_{row[0]}\"\n",
    "        if row[6].strip(): subject = URIRef(ISBN[row[6]])\n",
    "        else: subject = MYNS[id]\n",
    "        my_kg.add((subject, RDF.type, SCHEMA['Book']))\n",
    "        my_kg.add((subject, SCHEMA.archivedAt, Literal(\"https://www.barnesandnoble.com/\")))\n",
    "        my_kg.add((subject, SCHEMA.name, Literal(row[1])))\n",
    "        if row[6].strip(): my_kg.add((subject, SCHEMA.gtin13, Literal(row[6])))\n",
    "        if row[8].strip(): my_kg.add((subject, SCHEMA.numberOfPages, Literal(int(row[8]), datatype=XSD.integer)))\n",
    "        for i in [2,3,4]:\n",
    "            if row[i].strip():\n",
    "                my_kg.add((subject, SCHEMA.author, Literal(row[i], datatype=SCHEMA.Person)))\n",
    "        # Rating\n",
    "        if row[11].strip(): \n",
    "            my_kg.add((BNode(\"rating_\"+id), RDF.type, SCHEMA.AggregateRating))\n",
    "            my_kg.add((BNode(\"rating_\"+id), SCHEMA.ratingValue, Literal(row[12], datatype=XSD.float)))\n",
    "            my_kg.add((BNode(\"rating_\"+id), SCHEMA.ratingCount, Literal(row[11], datatype=XSD.integer)))\n",
    "            my_kg.add((subject, SCHEMA['aggregateRating'], BNode(\"rating_\"+id)))\n",
    "\n",
    "        if row[5].strip(): my_kg.add((subject, SCHEMA.publisher, Literal(row[5], datatype=SCHEMA.Organization)))\n",
    "        if row[7].strip(): my_kg.add((subject, SCHEMA.datePublished, Literal(row[7], datatype=SCHEMA.Date)))\n",
    "        if row[9].strip(): my_kg.add((subject, SCHEMA.size, Literal(row[9], datatype=SCHEMA.SizeSpecification)))\n",
    "        if row[10].strip(): my_kg.add((subject, MYNS.salesRank, Literal(int(row[10].replace(\",\",\"\")), datatype=XSD.integer)))\n",
    "        if row[13].strip(): my_kg.add((subject, MYNS.paperbackPrice, Literal(float(row[13].strip(\"$\")), datatype=XSD.dollar)))\n",
    "        if row[14].strip(): my_kg.add((subject, MYNS.hardcoverPrice, Literal(float(row[14].strip(\"$\")), datatype=XSD.dollar)))\n",
    "        if row[15].strip(): my_kg.add((subject, MYNS.nookBookPrice, Literal(float(row[15].strip(\"$\")), datatype=XSD.dollar)))\n",
    "        if row[16].strip(): my_kg.add((subject, MYNS.audiobookPrice, Literal(float(row[16].strip(\"$\")), datatype=XSD.dollar)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r1_id, r2_id in book_matchings_id:\n",
    "    r1 = good_ds.get_record(r1_id)\n",
    "    try:\n",
    "        _ = int(r1.ISBN13)\n",
    "        r1_subject = ISBN[r1.ISBN13]\n",
    "    except: r1_subject = MYNS[\"good_\"+r1_id]\n",
    "    r2  = noble_ds.get_record(r2_id)\n",
    "    try:\n",
    "        _ = int(r2.ISBN13)\n",
    "        r2_subject = ISBN[r2.ISBN13]\n",
    "    except: r2_subject = MYNS[\"noble_\"+r2_id]\n",
    "    my_kg.add((r1_subject, RDFS.seeAlso, r2_subject))\n",
    "    my_kg.add((r2_subject, RDFS.seeAlso, r1_subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N4f6a8586e873425a899379da34fb8302 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_kg.serialize('Ruijie_Rao_model.ttl', format=\"turtle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DS558')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d2ff20c00d4ac93ff3e3c3c2246f9d6a04344282adafe22c7a286cd6fb7db25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
